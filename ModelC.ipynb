{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is for ModelC\n",
    "build source model based on source countries data(CCPM and control measures) and predict CCPM for target countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:37.278207Z",
     "start_time": "2020-06-23T09:10:37.271399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Part 1 - Data Preprocessing\n",
    "import tensorflow as tf\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import datetime \n",
    "\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model,clone_model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense,  Lambda\n",
    "\n",
    "from keras.backend import slice\n",
    "from keras.constraints import max_norm\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='-1'\n",
    "\n",
    "##　locate the directory storing the data \n",
    "os.chdir(os.getcwd()+'/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:38.101209Z",
     "start_time": "2020-06-23T09:10:38.098096Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:38.711383Z",
     "start_time": "2020-06-23T09:10:38.699950Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences_x(data, seq_length):\n",
    "    xs = []\n",
    "    for i in range(len(data)-seq_length+1):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        xs.append(x)\n",
    "    return np.array(xs)\n",
    "\n",
    "\n",
    "def create_sequences_y(data, seq_length):\n",
    "    ys = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        y = data[i]\n",
    "        ys.append(y)\n",
    "    return np.array(ys)\n",
    "\n",
    "def MAPE(y, y_pred):\n",
    "    mape = sum(abs(y-y_pred)/y)/len(y)\n",
    "    print('MAPE: ', mape)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造source序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:39.760273Z",
     "start_time": "2020-06-23T09:10:39.756442Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from re import sub\n",
    "\n",
    "mypath = 'source/'\n",
    "source_countries = ['Austria','China (except Hubei)','Croatia','Germany','Hubei','Italy','Japan',\n",
    "            'Lebanon','Monaco','Norway','Oman','United Arab Emirates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:42.082833Z",
     "start_time": "2020-06-23T09:10:40.267571Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austria\n",
      "China (except Hubei)\n",
      "Croatia\n",
      "Germany\n",
      "Hubei\n",
      "Italy\n",
      "Japan\n",
      "Lebanon\n",
      "Monaco\n",
      "Norway\n",
      "Oman\n",
      "United Arab Emirates\n"
     ]
    }
   ],
   "source": [
    "pred_length = 7\n",
    "seq_length = 7\n",
    "x_seq0 = []\n",
    "y_seq0 = []\n",
    "\n",
    "for sc in source_countries:\n",
    "    print(sc)\n",
    "    # data preprocessing\n",
    "    df = pd.read_excel('./source/'+sc+'.xlsx', index_col=0)\n",
    "\n",
    "    df_new_1day = df[['confirmed cases per million']].diff(periods=1)\n",
    "    df_new_1day.rename(columns={'confirmed cases per million':'new cases'}, inplace=True)\n",
    "    df_new_1day['control'] = df.control.values\n",
    "\n",
    "    df_new_7days = df[['confirmed cases per million']].diff(periods=pred_length)\n",
    "    df_new_7days.rename(columns={'confirmed cases per million':'new cases'}, inplace=True)\n",
    "\n",
    "    df_new_7days['cum cases'] = 0\n",
    "    df_new_7days['cum cases'][pred_length:] = df['confirmed cases per million'].values[0:len(df_new_7days)-pred_length]  \n",
    "    \n",
    "    ## scale data\n",
    "    scaler_x = MinMaxScaler() #scale data into 0-1\n",
    "    scaler_y = MinMaxScaler() \n",
    "\n",
    "    if len(x_seq0)==0:\n",
    "        x_seq0 = create_sequences_x(np.array(df_new_1day.dropna()), seq_length)\n",
    "        y_seq0 = create_sequences_y(np.array(df_new_7days.dropna()), seq_length)\n",
    "        x_seq0 = x_seq0[0:len(y_seq0)]\n",
    "    else:\n",
    "        tx_seq0 = create_sequences_x(np.array(df_new_1day.dropna()), seq_length)\n",
    "        ty_seq0 = create_sequences_y(np.array(df_new_7days.dropna()), seq_length)\n",
    "        tx_seq0 = tx_seq0[0:len(ty_seq0)]\n",
    "        \n",
    "        x_seq0 = np.concatenate((x_seq0, tx_seq0),axis=0)\n",
    "        y_seq0 = np.concatenate((y_seq0, ty_seq0),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scale the sequence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:42.091288Z",
     "start_time": "2020-06-23T09:10:42.085218Z"
    }
   },
   "outputs": [],
   "source": [
    "x_seq1 = np.reshape(x_seq0, newshape=(-1,1))\n",
    "\n",
    "scaler_x = scaler_x.fit(x_seq1)\n",
    "x = scaler_x.transform(x_seq1)\n",
    "x = np.reshape(x, newshape=(x_seq0.shape))\n",
    "y_seq1 = np.reshape(y_seq0[:,0:1], newshape=(-1,1))\n",
    "\n",
    "scaler_y = scaler_y.fit(y_seq1)\n",
    "y = scaler_y.transform(y_seq1)\n",
    "\n",
    "y = np.reshape(y, newshape=(y_seq0[:,0:1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练source模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:42.157955Z",
     "start_time": "2020-06-23T09:10:42.155322Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:48.324954Z",
     "start_time": "2020-06-23T09:10:42.371930Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data1/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /data1/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 300 samples, validate on 593 samples\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.0323 - val_loss: 0.0227\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s 312us/step - loss: 0.0257 - val_loss: 0.0196\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s 308us/step - loss: 0.0224 - val_loss: 0.0190\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s 299us/step - loss: 0.0213 - val_loss: 0.0191\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s 273us/step - loss: 0.0209 - val_loss: 0.0186\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s 274us/step - loss: 0.0204 - val_loss: 0.0182\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s 263us/step - loss: 0.0198 - val_loss: 0.0175\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s 292us/step - loss: 0.0193 - val_loss: 0.0172\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s 263us/step - loss: 0.0188 - val_loss: 0.0168\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s 278us/step - loss: 0.0183 - val_loss: 0.0163\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s 262us/step - loss: 0.0179 - val_loss: 0.0160\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s 263us/step - loss: 0.0174 - val_loss: 0.0156\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s 264us/step - loss: 0.0169 - val_loss: 0.0153\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s 260us/step - loss: 0.0165 - val_loss: 0.0152\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s 241us/step - loss: 0.0161 - val_loss: 0.0148\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s 263us/step - loss: 0.0157 - val_loss: 0.0144\n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s 268us/step - loss: 0.0154 - val_loss: 0.0140\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s 270us/step - loss: 0.0150 - val_loss: 0.0139\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s 267us/step - loss: 0.0149 - val_loss: 0.0139\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s 261us/step - loss: 0.0145 - val_loss: 0.0135\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s 273us/step - loss: 0.0143 - val_loss: 0.0132\n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s 274us/step - loss: 0.0141 - val_loss: 0.0131\n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s 273us/step - loss: 0.0139 - val_loss: 0.0130\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s 268us/step - loss: 0.0138 - val_loss: 0.0129\n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s 263us/step - loss: 0.0136 - val_loss: 0.0129\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s 272us/step - loss: 0.0135 - val_loss: 0.0128\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s 278us/step - loss: 0.0134 - val_loss: 0.0126\n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s 271us/step - loss: 0.0134 - val_loss: 0.0127\n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s 282us/step - loss: 0.0134 - val_loss: 0.0127\n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s 270us/step - loss: 0.0133 - val_loss: 0.0124\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s 265us/step - loss: 0.0132 - val_loss: 0.0124\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s 261us/step - loss: 0.0131 - val_loss: 0.0127\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s 253us/step - loss: 0.0131 - val_loss: 0.0127\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s 261us/step - loss: 0.0131 - val_loss: 0.0125\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s 270us/step - loss: 0.0131 - val_loss: 0.0126\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s 276us/step - loss: 0.0129 - val_loss: 0.0123\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s 268us/step - loss: 0.0129 - val_loss: 0.0123\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s 263us/step - loss: 0.0129 - val_loss: 0.0123\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s 263us/step - loss: 0.0128 - val_loss: 0.0123\n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s 270us/step - loss: 0.0128 - val_loss: 0.0123\n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s 276us/step - loss: 0.0128 - val_loss: 0.0122\n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s 239us/step - loss: 0.0127 - val_loss: 0.0123\n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s 236us/step - loss: 0.0127 - val_loss: 0.0122\n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s 249us/step - loss: 0.0127 - val_loss: 0.0122\n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s 250us/step - loss: 0.0126 - val_loss: 0.0121\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s 260us/step - loss: 0.0126 - val_loss: 0.0122\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s 261us/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s 246us/step - loss: 0.0126 - val_loss: 0.0121\n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s 254us/step - loss: 0.0125 - val_loss: 0.0121\n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s 249us/step - loss: 0.0125 - val_loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "seed(123)\n",
    "train_idx = sample(range(len(x)),300)\n",
    "test_idx = set(range(len(x))).difference(train_idx)\n",
    "\n",
    "\n",
    "X_train = x[train_idx].copy()\n",
    "y_train = np.reshape(y[train_idx], (-1))\n",
    "\n",
    "X_test = x[list(test_idx)].copy()\n",
    "y_test = np.reshape(y[list(test_idx)], (-1))\n",
    "\n",
    "# Building the model\n",
    "main_input = Input(shape=(seq_length,2,), dtype='float32', name='main_input')  \n",
    "lstm_out = LSTM(4)(main_input)   \n",
    "main_output = Dense(units = 1)(lstm_out)\n",
    "regressor = Model(inputs=main_input, outputs=main_output)\n",
    "\n",
    "\n",
    "regressor.compile(optimizer='adam', loss='mse')\n",
    "regressor.fit(X_train, y_train, epochs = 50, batch_size = 32, validation_data=(X_test,y_test))\n",
    "regressor.save('../model/ModelC-control.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target countries - medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:48.332017Z",
     "start_time": "2020-06-23T09:10:48.327700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from re import sub\n",
    "# target_countries = ['Albania','Algeria','Argentina','Armenia','Australia','Azerbaijan','Bangladesh','Belarus','Belgium','Bermuda',\n",
    "#              'Bolivia','Brazil','Bulgaria','Canada','Chile','Colombia','Costa Rica','Cuba','Czech Republic','Denmark',\n",
    "#              'El Salvador','Estonia','Finland','France','Ghana','Gibraltar','Greece','Honduras','Hungary','India',\n",
    "#              'Indonesia','Iran','Iraq','Ireland','Israel','Jamaica','Jordan','Liberia','Luxembourg','Malaysia','Mexico',\n",
    "#              'Morocco','Nepal','Netherlands','Nigeria','Pakistan','Paraguay','Peru','Philippines','Poland','Portugal',\n",
    "#              'Qatar','Republic of the Congo','Romania','Russia','Rwanda','Saudi Arabia','Senegal','Sierra Leone',\n",
    "#              'Singapore','Slovakia','Slovenia','South Africa','Sri Lanka','Switzerland','Thailand','Tunisia','Turkey',\n",
    "#              'Ukraine','United Kingdom','United States','Venezuela']\n",
    "target_countries = ['Australia','France','Greece','Iraq','Netherlands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:10:50.032025Z",
     "start_time": "2020-06-23T09:10:48.337049Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_model = load_model('../model/ModelC-control.pkl')\n",
    "ret_test = pd.DataFrame(index=target_countries, columns=['MAPE'])\n",
    "modelc_control = pd.DataFrame(index=['loop1', 'loop2', 'loop3', 'loop4','loop5'], columns=['MAPE(mean)','MAPE(std)','MAPE<0.1','MAPE<0.05'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:11:04.364707Z",
     "start_time": "2020-06-23T09:10:50.034706Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia\n",
      "MAPE:  0.041625889435366\n",
      "France\n",
      "MAPE:  0.028971640956362954\n",
      "Greece\n",
      "MAPE:  0.033116221847490315\n",
      "Iraq\n",
      "MAPE:  0.1004357721459402\n",
      "Netherlands\n",
      "MAPE:  0.03850912715152976\n"
     ]
    }
   ],
   "source": [
    "mypath = 'target/'\n",
    "for l in range(1):\n",
    "    for tar in target_countries:\n",
    "        print(tar)\n",
    "        # data preprocessing\n",
    "        df = pd.read_excel(mypath+tar+'.xlsx', index_col=0)\n",
    "\n",
    "        df_new_1day = df.diff(periods=1)\n",
    "        df_new_1day.rename(columns={'confirmed cases per million':'new cases'}, inplace=True)\n",
    "\n",
    "        df_new_1day['control'] = df.control.values\n",
    "\n",
    "        df_new_7days = df[['confirmed cases per million']].diff(periods=pred_length)\n",
    "        df_new_7days.rename(columns={'confirmed cases per million':'new cases'}, inplace=True)\n",
    "\n",
    "        df_new_7days['cum cases'] = 0\n",
    "        df_new_7days['cum cases'][pred_length:] = df['confirmed cases per million'].values[0:len(df_new_7days)-pred_length]\n",
    "\n",
    "        x_seq0 = create_sequences_x(np.array(df_new_1day.dropna()), seq_length)\n",
    "        y_seq0 = create_sequences_y(np.array(df_new_7days.dropna()), seq_length)\n",
    "        x_seq0 = x_seq0[0:len(y_seq0)]\n",
    "\n",
    "        x_seq1 = np.reshape(x_seq0, newshape=(-1,1))\n",
    "        x = scaler_x.transform(x_seq1)\n",
    "\n",
    "        x = np.reshape(x, newshape=(x_seq0.shape))\n",
    "\n",
    "        y_seq1 = np.reshape(y_seq0[:,0:1], newshape=(-1,1))\n",
    "        y = scaler_y.transform(y_seq1)\n",
    "\n",
    "        y = np.reshape(y, newshape=(y_seq0[:,0:1].shape))\n",
    "\n",
    "        test_idx = int(len(x)*0.8)\n",
    "        X_train = x[0:test_idx, :, 0:2].copy()\n",
    "        y_train = np.reshape(y[0:test_idx], (-1))\n",
    "\n",
    "        X_test = x[test_idx:,:,0:2].copy()\n",
    "        y_test = np.reshape(y[test_idx:], (-1))\n",
    "\n",
    "        model =clone_model(pred_model)\n",
    "        model.set_weights(pred_model.get_weights())\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable=False\n",
    "        for layer in model.layers[-1:]:\n",
    "            layer.trainable=True\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # Fitting the RNN to the Training set\n",
    "        model.fit(X_train, y_train, epochs = 12, batch_size = 2, verbose=0)\n",
    "\n",
    "        # Predicting daily cases\n",
    "        predicted_cases = model.predict(X_test)\n",
    "        predicted_cases = scaler_y.inverse_transform(predicted_cases)\n",
    "\n",
    "        true_cases = np.reshape(scaler_y.inverse_transform(np.reshape(y_test,(-1,1))),(-1))+y_seq0[test_idx:,1]\n",
    "        predicted_cases = np.reshape(predicted_cases,(-1))+y_seq0[test_idx:,1]\n",
    "\n",
    "\n",
    "        mape = MAPE(true_cases, predicted_cases) \n",
    "        ret_test.loc[tar, 'MAPE'] = mape\n",
    "    modelc_control.loc['loop'+str(l+1), 'MAPE(mean)'] = ret_test.MAPE.mean() \n",
    "    modelc_control.loc['loop'+str(l+1), 'MAPE(std)'] = ret_test.MAPE.std()\n",
    "    modelc_control.loc['loop'+str(l+1), 'MAPE<0.1'] = ret_test[(ret_test.MAPE<0.1)].shape[0]\n",
    "    modelc_control.loc['loop'+str(l+1), 'MAPE<0.05']=ret_test[(ret_test.MAPE<0.05)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T09:12:06.667680Z",
     "start_time": "2020-06-23T09:12:06.663579Z"
    }
   },
   "outputs": [],
   "source": [
    "# modelc_control.to_csv('../result/modelC_control.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
